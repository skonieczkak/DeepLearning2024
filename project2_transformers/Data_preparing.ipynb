{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import Wav2Vec2Model, BertConfig, BertModel\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class SpeechCommandsDataset(Dataset):\n",
    "    def __init__(self, dataset_path, background_noise_path, segment_length=1, unknown_label='unknown', target_background_samples=2375, selected_words=None):\n",
    "        self.selected_words = selected_words if selected_words is not None else []\n",
    "        self.audio_labels = []\n",
    "        self.audio_paths = []\n",
    "        self.label_to_index = {unknown_label: 0}\n",
    "        self.total_size_bytes = 0\n",
    "        self.target_background_samples = target_background_samples\n",
    "        self.background_samples_added = 0\n",
    "        all_labels = sorted(os.listdir(dataset_path))\n",
    "\n",
    "        for label in all_labels:\n",
    "            if label.startswith('_') or (self.selected_words and label not in self.selected_words):\n",
    "                continue  \n",
    "            label_path = os.path.join(dataset_path, label)\n",
    "            if os.path.isdir(label_path):\n",
    "                label_index = len(self.label_to_index)\n",
    "                self.label_to_index[label] = label_index\n",
    "\n",
    "                for audio_file in os.listdir(label_path):\n",
    "                    if audio_file.endswith('.wav'):\n",
    "                        file_path = os.path.join(label_path, audio_file)\n",
    "                        self.audio_paths.append(file_path)\n",
    "                        self.audio_labels.append(label_index)\n",
    "                        self.total_size_bytes += os.path.getsize(file_path)\n",
    "\n",
    "        # noise_files = os.listdir(background_noise_path)\n",
    "        # random.shuffle(noise_files)  \n",
    "        # for noise_file in noise_files:\n",
    "        #     if noise_file.endswith('.wav'):\n",
    "        #         noise_path = os.path.join(background_noise_path, noise_file)\n",
    "        #         if self.background_samples_added < self.target_background_samples:\n",
    "        #             self.process_background_noise(noise_path, segment_length)\n",
    "\n",
    "    # def process_background_noise(self, noise_path, segment_length):\n",
    "    #     waveform, sample_rate = torchaudio.load(noise_path)\n",
    "    #     total_samples = waveform.size(1)\n",
    "    #     samples_per_segment = int(sample_rate * segment_length)\n",
    "    #     remaining_segments = self.target_background_samples - self.background_samples_added\n",
    "        \n",
    "    #     while remaining_segments > 0:\n",
    "    #         max_start = total_samples - samples_per_segment\n",
    "    #         start = random.randint(0, max_start)\n",
    "    #         end = start + samples_per_segment\n",
    "            \n",
    "    #         self.audio_paths.append((noise_path, start, end))\n",
    "    #         self.audio_labels.append(self.label_to_index['unknown'])\n",
    "    #         self.total_size_bytes += end - start\n",
    "    #         self.background_samples_added += 1\n",
    "    #         remaining_segments -= 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_info = self.audio_paths[idx]\n",
    "        label = self.audio_labels[idx]\n",
    "        if isinstance(audio_info, tuple):\n",
    "            waveform, sample_rate = torchaudio.load(audio_info[0], frame_offset=audio_info[1], num_frames=audio_info[2]-audio_info[1])\n",
    "        else:\n",
    "            waveform, sample_rate = torchaudio.load(audio_info)\n",
    "        waveform = waveform.squeeze()\n",
    "        if waveform.numel() == 0:  # Check if the waveform is empty\n",
    "            print(f\"Empty waveform at idx: {idx}, path: {audio_info}\")\n",
    "        return waveform, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, wav2vec_model_name, num_labels, learning_rate=0.001, weight_decay=0.01):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "        self.wav2vec = Wav2Vec2Model.from_pretrained(wav2vec_model_name)\n",
    "        \n",
    "        transformer_config = BertConfig(\n",
    "            hidden_size=self.wav2vec.config.hidden_size,\n",
    "            num_attention_heads=self.wav2vec.config.num_attention_heads,\n",
    "            num_hidden_layers=1, \n",
    "        )\n",
    "        self.transformer = BertModel(transformer_config)\n",
    "    \n",
    "        self.classifier = nn.Linear(self.wav2vec.config.hidden_size, num_labels)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.scheduler = LambdaLR(self.optimizer, lr_lambda=lambda epoch: 0.7 ** (epoch // 5))\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        with torch.no_grad():\n",
    "            extracted_features = self.wav2vec(input_values).last_hidden_state\n",
    "        transformer_output = self.transformer(inputs_embeds=extracted_features)\n",
    "        cls_output = transformer_output.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(cls_output)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def fit(self, train_loader, epochs, device):\n",
    "        self.to(device)\n",
    "        self.train()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # tqdm is used here for the progress bar in the epochs loop\n",
    "            with tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}', unit='batch') as tepoch:\n",
    "                for input_values, labels in tepoch:\n",
    "                    input_values = input_values.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    outputs = self(input_values)\n",
    "\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                    \n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    tepoch.set_postfix(loss=loss.item())\n",
    "                    self.scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2377\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = './train/train/audio'\n",
    "BACKGROUND_NOISE_PATH = './train/train/audio/_background_noise_'\n",
    "dataset = SpeechCommandsDataset(DATASET_PATH, BACKGROUND_NOISE_PATH)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(dataset.audio_labels.count(dataset.label_to_index['unknown']))\n",
    "print(dataset.audio_labels.count(dataset.label_to_index['yes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51088 6798 6835\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "MAX_NUM_WAVS_PER_CLASS = 2**27 - 1  \n",
    "\n",
    "def which_set(filename, validation_percentage, testing_percentage):\n",
    "    base_name = os.path.basename(filename)\n",
    "    hash_name = re.sub(r'_nohash_.*$', '', base_name)\n",
    "    hash_name_hashed = hashlib.sha1(hash_name.encode('utf-8')).hexdigest()\n",
    "    percentage_hash = ((int(hash_name_hashed, 16) % (MAX_NUM_WAVS_PER_CLASS + 1)) *\n",
    "                       (100.0 / MAX_NUM_WAVS_PER_CLASS))\n",
    "    if percentage_hash < validation_percentage:\n",
    "        result = 'validation'\n",
    "    elif percentage_hash < (testing_percentage + validation_percentage):\n",
    "        result = 'testing'\n",
    "    else:\n",
    "        result = 'training'\n",
    "    return result\n",
    "\n",
    "validation_percentage = 10.0 \n",
    "testing_percentage = 10.0 \n",
    "\n",
    "train_files = []\n",
    "validation_files = []\n",
    "test_files = []\n",
    "\n",
    "for file_path in dataset.audio_paths:\n",
    "    set_category = which_set(file_path, validation_percentage, testing_percentage)\n",
    "    if set_category == 'training':\n",
    "        train_files.append(file_path)\n",
    "    elif set_category == 'validation':\n",
    "        validation_files.append(file_path)\n",
    "    else:  # 'testing'\n",
    "        test_files.append(file_path)\n",
    "\n",
    "train_files[:5], validation_files[:5], test_files[:5] \n",
    "\n",
    "print(len(train_files), len(validation_files) , len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51088 6798 6835\n"
     ]
    }
   ],
   "source": [
    "print(len(train_files), len(validation_files) , len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5573, 792, 764)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Get label indices for 'yes' and 'no'\n",
    "yes_index = dataset.label_to_index['yes']\n",
    "no_index = dataset.label_to_index['no']\n",
    "\n",
    "# Initialize the lists for storing file paths\n",
    "train_yes = []\n",
    "train_no = []\n",
    "train_unknown = []\n",
    "validation_yes = []\n",
    "validation_no = []\n",
    "validation_unknown = []\n",
    "test_yes = []\n",
    "test_no = []\n",
    "test_unknown = []\n",
    "\n",
    "# Distribute files into the respective sets based on the provided function\n",
    "for file_path, label in zip(dataset.audio_paths, dataset.audio_labels):\n",
    "    set_type = which_set(file_path, validation_percentage, testing_percentage)\n",
    "    if label == yes_index:\n",
    "        if set_type == 'training':\n",
    "            train_yes.append(file_path)\n",
    "        elif set_type == 'validation':\n",
    "            validation_yes.append(file_path)\n",
    "        else:\n",
    "            test_yes.append(file_path)\n",
    "    elif label == no_index:\n",
    "        if set_type == 'training':\n",
    "            train_no.append(file_path)\n",
    "        elif set_type == 'validation':\n",
    "            validation_no.append(file_path)\n",
    "        else:\n",
    "            test_no.append(file_path)\n",
    "    else:\n",
    "        if set_type == 'training':\n",
    "            train_unknown.append(file_path)\n",
    "        elif set_type == 'validation':\n",
    "            validation_unknown.append(file_path)\n",
    "        else:\n",
    "            test_unknown.append(file_path)\n",
    "\n",
    "num_yes_train = len(train_yes)\n",
    "num_yes_test = len(test_yes)\n",
    "num_yes_validation = len(validation_yes)\n",
    "random.shuffle(train_unknown)\n",
    "random.shuffle(train_unknown)\n",
    "random.shuffle(train_unknown)\n",
    "train_unknown = train_unknown[:num_yes_train]\n",
    "test_unknown = test_unknown[:num_yes_test]\n",
    "validation_unknown = validation_unknown[:num_yes_validation]\n",
    "\n",
    "train_files = train_yes + train_no + train_unknown\n",
    "validation_files = validation_yes + validation_no + validation_unknown\n",
    "test_files = test_yes + test_no + test_unknown\n",
    "\n",
    "random.shuffle(train_files)\n",
    "random.shuffle(validation_files)\n",
    "random.shuffle(test_files)\n",
    "\n",
    "len(train_files), len(validation_files), len(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torchaudio\n",
    "\n",
    "import os\n",
    "import random\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "def process_and_distribute_background_noise(background_noise_path, segment_length, target_background_samples, num_train_samples, num_val_samples, num_test_samples):\n",
    "    noise_files = [f for f in os.listdir(background_noise_path) if f.endswith('.wav')]\n",
    "    random.shuffle(noise_files)\n",
    "    \n",
    "    audio_segments = []\n",
    "    label_to_index = {'unknown': 0}  # Assuming 'unknown' is the label for background noise\n",
    "    \n",
    "    total_required_samples = num_train_samples + num_val_samples + num_test_samples\n",
    "\n",
    "    background_samples_added = 0\n",
    "    while background_samples_added < target_background_samples:\n",
    "        noise_file = random.choice(noise_files)\n",
    "        noise_path = os.path.join(background_noise_path, noise_file)\n",
    "        waveform, sample_rate = torchaudio.load(noise_path)\n",
    "        total_samples = waveform.size(1)\n",
    "        samples_per_segment = int(sample_rate * segment_length)\n",
    "\n",
    "        max_start = total_samples - samples_per_segment\n",
    "        if max_start > 0: \n",
    "            start = random.randint(0, max_start)\n",
    "            end = start + samples_per_segment\n",
    "\n",
    "            audio_segments.append((noise_path, start, end, label_to_index['unknown']))\n",
    "            background_samples_added += 1\n",
    "\n",
    "    random.shuffle(audio_segments)\n",
    "\n",
    "    total_collected_samples = len(audio_segments)\n",
    "\n",
    "    if total_required_samples > total_collected_samples:\n",
    "        raise ValueError(\"Requested number of samples exceeds the total number of collected samples.\")\n",
    "\n",
    "    train_segments = audio_segments[:num_train_samples]\n",
    "    val_segments = audio_segments[num_train_samples:num_train_samples + num_val_samples]\n",
    "    test_segments = audio_segments[num_train_samples + num_val_samples:num_train_samples + num_val_samples + num_test_samples]\n",
    "\n",
    "    return train_segments, val_segments, test_segments\n",
    "\n",
    "# Example usage\n",
    "background_noise_path = BACKGROUND_NOISE_PATH\n",
    "segment_length = 1  \n",
    "target_background_samples = num_yes_train+num_yes_test+num_yes_validation \n",
    "num_train_samples = num_yes_train\n",
    "num_val_samples = num_yes_test\n",
    "num_test_samples = num_yes_validation\n",
    "\n",
    "train_unknow, val_unknown, test_unknown = process_and_distribute_background_noise(\n",
    "    background_noise_path, segment_length, target_background_samples, \n",
    "    num_train_samples, num_val_samples, num_test_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    audios, labels = zip(*batch)\n",
    "    audios_padded = pad_sequence([audio for audio in audios], batch_first=True, padding_value=0.0)\n",
    "    labels = torch.tensor(labels)\n",
    "    return audios_padded, labels\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/10:   0%|          | 1/2023 [00:01<48:41,  1.44s/batch, loss=3.63]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[39m=\u001b[39m AudioClassifier(\u001b[39m'\u001b[39m\u001b[39mfacebook/wav2vec2-base-960h\u001b[39m\u001b[39m'\u001b[39m, num_labels\u001b[39m=\u001b[39m\u001b[39m31\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_loader, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[24], line 38\u001b[0m, in \u001b[0;36mAudioClassifier.fit\u001b[1;34m(self, train_loader, epochs, device)\u001b[0m\n\u001b[0;32m     36\u001b[0m input_values \u001b[39m=\u001b[39m input_values\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     37\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 38\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(input_values)\n\u001b[0;32m     40\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(outputs, labels)\n\u001b[0;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\kacperskonieczka\\.conda\\envs\\inz\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kacperskonieczka\\.conda\\envs\\inz\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[24], line 21\u001b[0m, in \u001b[0;36mAudioClassifier.forward\u001b[1;34m(self, input_values)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_values):\n\u001b[0;32m     20\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 21\u001b[0m         extracted_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwav2vec(input_values)\u001b[39m.\u001b[39mlast_hidden_state\n\u001b[0;32m     22\u001b[0m     transformer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer(inputs_embeds\u001b[39m=\u001b[39mextracted_features)\n\u001b[0;32m     23\u001b[0m     cls_output \u001b[39m=\u001b[39m transformer_output\u001b[39m.\u001b[39mlast_hidden_state[:, \u001b[39m0\u001b[39m, :]\n",
      "File \u001b[1;32mc:\\Users\\kacperskonieczka\\.conda\\envs\\inz\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kacperskonieczka\\.conda\\envs\\inz\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kacperskonieczka\\.conda\\envs\\inz\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1548\u001b[0m, in \u001b[0;36mWav2Vec2Model.forward\u001b[1;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1543\u001b[0m output_hidden_states \u001b[39m=\u001b[39m (\n\u001b[0;32m   1544\u001b[0m     output_hidden_states \u001b[39mif\u001b[39;00m output_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39moutput_hidden_states\n\u001b[0;32m   1545\u001b[0m )\n\u001b[0;32m   1546\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1548\u001b[0m extract_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_extractor(input_values)\n\u001b[0;32m   1549\u001b[0m extract_features \u001b[39m=\u001b[39m extract_features\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m   1551\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1552\u001b[0m     \u001b[39m# compute reduced attention_mask corresponding to feature vectors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kacperskonieczka\\.conda\\envs\\inz\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kacperskonieczka\\.conda\\envs\\inz\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kacperskonieczka\\.conda\\envs\\inz\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:455\u001b[0m, in \u001b[0;36mWav2Vec2FeatureEncoder.forward\u001b[1;34m(self, input_values)\u001b[0m\n\u001b[0;32m    450\u001b[0m         hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    451\u001b[0m             conv_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[0;32m    452\u001b[0m             hidden_states,\n\u001b[0;32m    453\u001b[0m         )\n\u001b[0;32m    454\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 455\u001b[0m         hidden_states \u001b[39m=\u001b[39m conv_layer(hidden_states)\n\u001b[0;32m    457\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\kacperskonieczka\\.conda\\envs\\inz\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kacperskonieczka\\.conda\\envs\\inz\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kacperskonieczka\\.conda\\envs\\inz\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:308\u001b[0m, in \u001b[0;36mWav2Vec2NoLayerNormConvLayer.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[1;32m--> 308\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(hidden_states)\n\u001b[0;32m    309\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(hidden_states)\n\u001b[0;32m    310\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\kacperskonieczka\\.conda\\envs\\inz\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kacperskonieczka\\.conda\\envs\\inz\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kacperskonieczka\\.conda\\envs\\inz\\lib\\site-packages\\torch\\nn\\modules\\conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 310\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\kacperskonieczka\\.conda\\envs\\inz\\lib\\site-packages\\torch\\nn\\modules\\conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    303\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    304\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    305\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 306\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    307\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = AudioClassifier('facebook/wav2vec2-base-960h', num_labels=31)\n",
    "model.fit(train_loader, epochs=10, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
