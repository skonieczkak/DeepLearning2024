{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import os\n",
    "import random\n",
    "import hashlib\n",
    "import torchaudio\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "class SpeechCommandsDataset(Dataset):\n",
    "    def __init__(self, dataset_path, unknown_label='unknown', target_background_samples=2375, selected_words=None):\n",
    "        self.selected_words = selected_words if selected_words is not None else []\n",
    "        self.audio_labels = []\n",
    "        self.audio_paths = []\n",
    "        self.label_to_index = {unknown_label: 0}\n",
    "        self.total_size_bytes = 0\n",
    "        self.target_background_samples = target_background_samples\n",
    "        self.background_samples_added = 0\n",
    "        all_labels = sorted(os.listdir(dataset_path))\n",
    "\n",
    "        for label in all_labels:\n",
    "            if label.startswith('_') or (self.selected_words and label not in self.selected_words):\n",
    "                continue  \n",
    "            label_path = os.path.join(dataset_path, label)\n",
    "            if os.path.isdir(label_path):\n",
    "                label_index = len(self.label_to_index)\n",
    "                self.label_to_index[label] = label_index\n",
    "\n",
    "                for audio_file in os.listdir(label_path):\n",
    "                    if audio_file.endswith('.wav'):\n",
    "                        file_path = os.path.join(label_path, audio_file)\n",
    "                        self.audio_paths.append(file_path)\n",
    "                        self.audio_labels.append(label_index)\n",
    "                        self.total_size_bytes += os.path.getsize(file_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_info = self.audio_paths[idx]\n",
    "        label = self.audio_labels[idx]\n",
    "        if isinstance(audio_info, tuple):\n",
    "            waveform, sample_rate = torchaudio.load(audio_info[0], frame_offset=audio_info[1], num_frames=audio_info[2]-audio_info[1])\n",
    "        else:\n",
    "            waveform, sample_rate = torchaudio.load(audio_info)\n",
    "        waveform = waveform.squeeze()\n",
    "        if waveform.numel() == 0:  # Check if the waveform is empty\n",
    "            print(f\"Empty waveform at idx: {idx}, path: {audio_info}\")\n",
    "        return waveform, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WAVS_PER_CLASS = 2**27 - 1\n",
    "\n",
    "def which_set(filename, validation_percentage, testing_percentage):\n",
    "    \"\"\"Determine the dataset split based on filename hashing.\"\"\"\n",
    "    base_name = os.path.basename(filename)\n",
    "    hash_name = re.sub(r'_nohash_.*$', '', base_name)\n",
    "    hash_name_hashed = hashlib.sha1(hash_name.encode('utf-8')).hexdigest()\n",
    "    percentage_hash = (int(hash_name_hashed, 16) % (MAX_NUM_WAVS_PER_CLASS + 1)) * (100.0 / MAX_NUM_WAVS_PER_CLASS)\n",
    "    \n",
    "    if percentage_hash < validation_percentage:\n",
    "        return 'validation'\n",
    "    elif percentage_hash < (testing_percentage + validation_percentage):\n",
    "        return 'testing'\n",
    "    else:\n",
    "        return 'training'\n",
    "\n",
    "def distribute_files_by_label(dataset, validation_percentage, testing_percentage):\n",
    "    \"\"\"Distribute files into training, validation, and testing sets based on labels.\"\"\"\n",
    "    label_indices = {\n",
    "        'yes': dataset.label_to_index['yes'],\n",
    "        'no': dataset.label_to_index['no'],\n",
    "        'unknown': dataset.label_to_index.get('unknown', -1)  # Default -1 if 'unknown' is not predefined\n",
    "    }\n",
    "    sets = {label: {'training': [], 'validation': [], 'testing': []} for label in label_indices}\n",
    "\n",
    "    for file_path, label in zip(dataset.audio_paths, dataset.audio_labels):\n",
    "        set_type = which_set(file_path, validation_percentage, testing_percentage)\n",
    "        for label_name, index in label_indices.items():\n",
    "            if label == index:\n",
    "                sets[label_name][set_type].append(file_path)\n",
    "                break\n",
    "\n",
    "    return sets\n",
    "\n",
    "def balance_and_shuffle_files(sets):\n",
    "    \"\"\"Balance the number of unknown samples to match the number of yes samples, then shuffle each set.\"\"\"\n",
    "    num_yes_train = len(sets['yes']['training'])\n",
    "    num_yes_validation = len(sets['yes']['validation'])\n",
    "    num_yes_testing = len(sets['yes']['testing'])\n",
    "\n",
    "    random.shuffle(sets['unknown']['training'])\n",
    "    random.shuffle(sets['unknown']['validation'])\n",
    "    random.shuffle(sets['unknown']['testing'])\n",
    "\n",
    "    sets['unknown']['training'] = sets['unknown']['training'][:num_yes_train]\n",
    "    sets['unknown']['validation'] = sets['unknown']['validation'][:num_yes_validation]\n",
    "    sets['unknown']['testing'] = sets['unknown']['testing'][:num_yes_testing]\n",
    "\n",
    "    train_files = sets['yes']['training'] + sets['no']['training'] + sets['unknown']['training']\n",
    "    validation_files = sets['yes']['validation'] + sets['no']['validation'] + sets['unknown']['validation']\n",
    "    test_files = sets['yes']['testing'] + sets['no']['testing'] + sets['unknown']['testing']\n",
    "\n",
    "    random.shuffle(train_files)\n",
    "    random.shuffle(validation_files)\n",
    "    random.shuffle(test_files)\n",
    "\n",
    "    return train_files, validation_files, test_files\n",
    "\n",
    "def process_background_noise(background_noise_path, segment_length, target_samples, num_train, num_val, num_test):\n",
    "    \"\"\"Process and distribute background noise into dataset segments.\"\"\"\n",
    "    noise_files = [f for f in os.listdir(background_noise_path) if f.endswith('.wav')]\n",
    "    random.shuffle(noise_files)\n",
    "    \n",
    "    audio_segments = []\n",
    "    while len(audio_segments) < target_samples and noise_files:\n",
    "        noise_file = random.choice(noise_files)\n",
    "        noise_path = os.path.join(background_noise_path, noise_file)\n",
    "        waveform, sample_rate = torchaudio.load(noise_path)\n",
    "        total_samples = waveform.size(1)\n",
    "        samples_per_segment = int(sample_rate * segment_length)\n",
    "        max_start = total_samples - samples_per_segment\n",
    "        \n",
    "        if max_start > 0:\n",
    "            start = random.randint(0, max_start)\n",
    "            end = start + samples_per_segment\n",
    "            audio_segments.append((noise_path, start, end))\n",
    "            if len(audio_segments) >= target_samples:\n",
    "                break\n",
    "\n",
    "    if len(audio_segments) < (num_train + num_val + num_test):\n",
    "        raise ValueError(\"Not enough samples collected to meet the required distribution.\")\n",
    "\n",
    "    random.shuffle(audio_segments)\n",
    "    train = audio_segments[:num_train]\n",
    "    val = audio_segments[num_train:num_train + num_val]\n",
    "    test = audio_segments[num_train + num_val:num_train + num_val + num_test]\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "def save_segments(segments, save_path, prefix):\n",
    "    \"\"\"Save segments of audio to disk.\"\"\"\n",
    "    saved_paths = []\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    for i, (path, start, end) in enumerate(segments):\n",
    "        waveform, sample_rate = torchaudio.load(path, frame_offset=start, num_frames=end-start)\n",
    "        segment_path = os.path.join(save_path, f\"{prefix}_{i}.wav\")\n",
    "        torchaudio.save(segment_path, waveform, sample_rate)\n",
    "        saved_paths.append(segment_path)\n",
    "    return saved_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training files count: 3713\n",
      "Validation files count: 531\n",
      "Testing files count: 508\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = './train/train/audio'\n",
    "BACKGROUND_NOISE_PATH = './train/train/audio/_background_noise_'\n",
    "dataset = SpeechCommandsDataset(DATASET_PATH, BACKGROUND_NOISE_PATH)\n",
    "\n",
    "# Set distribution percentages\n",
    "validation_percentage = 10.0\n",
    "testing_percentage = 10.0\n",
    "\n",
    "# Distribute files\n",
    "distributed_sets = distribute_files_by_label(dataset, validation_percentage, testing_percentage)\n",
    "\n",
    "# Balance and shuffle unknown samples to match the number of yes samples\n",
    "train_files, validation_files, test_files = balance_and_shuffle_files(distributed_sets)\n",
    "\n",
    "# Display the result counts\n",
    "print(\"Training files count:\", len(train_files))\n",
    "print(\"Validation files count:\", len(validation_files))\n",
    "print(\"Testing files count:\", len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_count_train = sum(1 for file_path in train_files if 'yes' in file_path.split(os.sep)[-2])\n",
    "yes_count_val = sum(1 for file_path in validation_files if 'yes' in file_path.split(os.sep)[-2])\n",
    "yes_count_test = sum(1 for file_path in test_files if 'yes' in file_path.split(os.sep)[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_background_samples = len(train_files)\n",
    "num_train_samples = yes_count_train\n",
    "num_val_samples = yes_count_val\n",
    "num_test_samples = yes_count_test\n",
    "segment_length=1\n",
    "\n",
    "train_unknown, val_unknown, test_unknown = process_background_noise(\n",
    "    BACKGROUND_NOISE_PATH, segment_length, target_background_samples, \n",
    "    num_train_samples, num_val_samples, num_test_samples\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unknown_save_path = './train/train/audio/silence/train'\n",
    "val_unknown_save_path = './train/train/audio/silence/val'\n",
    "test_unknown_save_path = './train/train/audio/silence/test'\n",
    "\n",
    "train_silence_files = save_segments(train_unknown, train_unknown_save_path, 'train_silence')\n",
    "val_silence_files = save_segments(val_unknown, val_unknown_save_path, 'val_silence')\n",
    "test_silence_files = save_segments(test_unknown, test_unknown_save_path, 'test_silence')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files_set = set(train_files)\n",
    "validation_files_set = set(validation_files)\n",
    "test_files_set = set(test_files)\n",
    "\n",
    "train_files_set.update(train_silence_files)\n",
    "validation_files_set.update(val_silence_files)\n",
    "test_files_set.update(test_silence_files)\n",
    "\n",
    "train_files = list(train_files_set)\n",
    "validation_files = list(validation_files_set)\n",
    "test_files = list(test_files_set)\n",
    "\n",
    "random.shuffle(train_files)\n",
    "random.shuffle(validation_files)\n",
    "random.shuffle(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training files count: 5573\n",
      "Final validation files count: 792\n",
      "Final testing files count: 764\n"
     ]
    }
   ],
   "source": [
    "print(\"Final training files count:\", len(train_files))\n",
    "print(\"Final validation files count:\", len(validation_files))\n",
    "print(\"Final testing files count:\", len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, file_paths):\n",
    "        self.file_paths = file_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform, sample_rate = torchaudio.load(self.file_paths[idx])\n",
    "        return waveform, sample_rate\n",
    "\n",
    "train_dataset = AudioDataset(train_files)\n",
    "validation_dataset = AudioDataset(validation_files)\n",
    "test_dataset = AudioDataset(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
